{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models in Earth-2 MIP\n",
    "\n",
    "The following notebook demonstrates how to use Earth-2 MIP for running different AI weather models and comparing their outputs. Specifically, this will compare the Pangu weather model and Deep Learning Weather Prediction (DLWP) mode with an intial state pulled from the Climate Data Store (CDS). This will also how how to interact with Earth-2 MIP using Python APIs for greater control over inference workflows\n",
    "\n",
    "In summary this notebook will cover the following topics:\n",
    "\n",
    "- Configuring and setting up Pangu Model Registry and DLWP Model Registry\n",
    "- Setting up a basic deterministic inferencer for both models\n",
    "- Running inference in a Python script\n",
    "- Post processing results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "Starting off with imports, hopefully you have already installed Earth-2 MIP from this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For running inference we dont need much\n",
    "import os, json, logging, datetime\n",
    "import xarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to importing Earth-2 MIP, its critical we set up a few enviroment variables which will help Earth-2 MIP get configured correctly under the hood. There are a number of different global configuration options, the ones will will set here are:\n",
    "\n",
    "- `WORLD_SIZE`: Tells Earth-2 MIP (which uses Modulus under the hood) the number of GPUs present\n",
    "- `MODEL_REGISTRY`: This variable tells Earth-2 MIP where look for a model registery\n",
    "\n",
    "For addition information on the concept of model registry in Earth-2 MIP, have a look at the following notebooks with some additional information:\n",
    "\n",
    "- [01_ensemble_inference](./01_ensemble_inference.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of GPUs to use to 1\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "# Set model registry as a local folder\n",
    "model_registry = os.path.join(os.path.dirname(os.path.realpath(os. getcwd())), \"models\")\n",
    "os.makedirs(model_registry, exist_ok=True)\n",
    "os.environ['MODEL_REGISTRY'] = model_registry\n",
    "\n",
    "# With the enviroment variables set now we import Earth-2 MIP\n",
    "from earth2mip import registry, inference_ensemble\n",
    "from earth2mip.initial_conditions import cds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above created a model registry folder for us, now we need to populate it with model packages. We will start with the Pangu weather model by fetching the ONNX checkpoints and creating the `metadata.json`. This metadata JSON file will help Earth-2 MIP interact with the model checkpoint. Specifically, using a Python entry point. This will be discussed in more detail in later notebooks, but fundementally this tells Earth-2 MIP what load function to call for this model (this load function is found in `earth2mip/networks/pangu.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set up a pangu folder\n",
    "import subprocess\n",
    "if not os.path.isdir(os.path.join(model_registry, 'pangu')):\n",
    "    pangu_registry = os.path.join(model_registry, \"pangu\")\n",
    "    os.makedirs(pangu_registry, exist_ok=True)\n",
    "    # Wget onnx files\n",
    "    print(\"Downloading model checkpoint, this may take a bit\")\n",
    "    subprocess.run(['wget', '-nc', '-P', f'{pangu_registry}', 'https://get.ecmwf.int/repository/test-data/ai-models/pangu-weather/pangu_weather_24.onnx'], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    subprocess.run(['wget', '-nc', '-P', f'{pangu_registry}', 'https://get.ecmwf.int/repository/test-data/ai-models/pangu-weather/pangu_weather_6.onnx'], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "\n",
    "    with open(os.path.join(pangu_registry, 'metadata.json'), 'w') as outfile:\n",
    "        json.dump({\"entrypoint\": {\"name\": \"earth2mip.networks.pangu:load\"}}, outfile, indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next DLWP model package will need to be downloaded. This model follows the standard proceedure most do in Earth-2 MIP, being served via Modulus and hosted on NGC model registry. The install process is simple and all required files are present in the downloaded zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set up DLWP folder\n",
    "if not os.path.isdir(os.path.join(model_registry, 'dlwp')):\n",
    "    print(\"Downloading model checkpoint, this may take a bit\")\n",
    "    subprocess.run(['wget', '-nc', '-P', f'{model_registry}', 'https://api.ngc.nvidia.com/v2/models/nvidia/modulus/modulus_dlwp_cubesphere/versions/v0.2/files/dlwp_cubesphere.zip'], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    subprocess.run(['unzip', '-u', f'{model_registry}/dlwp_cubesphere.zip', '-d', f'{model_registry}'])\n",
    "    subprocess.run(['rm', f'{model_registry}/dlwp_cubesphere.zip'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final setup step is to set up your CDS API key so we can access ERA5 data to act as an initial state. Earth-2 MIP supports a number of different initial state data sources that are supported including HDF5, CDS, GFS, etc. The CDS initial state provides a convient way to access a limited amount of historical weather data. Its recommended for accessing an initial state, but larger data requirements should use locally stored weather datasets.\n",
    "\n",
    "Enter your CDS API uid and key below (found under your profile page). If you don't a CDS API key, find out more here.\n",
    "- [https://cds.climate.copernicus.eu/cdsapp#!/home](https://cds.climate.copernicus.eu/cdsapp#!/home)\n",
    "- [https://cds.climate.copernicus.eu/api-how-to](https://cds.climate.copernicus.eu/api-how-to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and input your credentials in the notebook\n",
    "cds_api = os.path.join(os.path.expanduser(\"~\"), '.cdsapirc')\n",
    "if not os.path.exists(cds_api):\n",
    "    uid = input(\"Enter in CDS UID (e.g. 654321)\")\n",
    "    key = input(\"Enter your CDS API key (e.g. 12345678-1234-1234-1234-123456123456)\")\n",
    "    # Write to config file for CDS library\n",
    "    with open(cds_api, 'w') as f:\n",
    "        f.write('url: https://cds.climate.copernicus.eu/api/v2\\n')\n",
    "        f.write(f'key: {uid}:{key}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference\n",
    "\n",
    "To run inference of these models we will use some of Earth-2 MIPs Python APIs to perform inference.\n",
    "The first step is to load the model from the model registry, which is done using the `registry.get_model` command.\n",
    "This will look in your `MODEL_REGISTRY` folder for the provided name and use this as a filesystem for loading necessary files.\n",
    "\n",
    "The model is then loaded into memory using the load function for that particular network. Earth-2 MIP has multiple abstracts that can allow this to be automated that can be used instead if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import earth2mip.networks.dlwp as dlwp\n",
    "import earth2mip.networks.pangu as pangu\n",
    "\n",
    "# Load DLWP model from registry\n",
    "package = registry.get_model(\"dlwp\")\n",
    "dlwp_inference_model = dlwp.load(package)\n",
    "\n",
    "# Load Pangu model(s) from registry\n",
    "package = registry.get_model(\"pangu\")\n",
    "pangu_inference_model = pangu.load(package)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up the initial state data source for January 1st, 2018 at 00:00:00 UTC. As previously mentioned, we will pull data on the fly from CDS (make sure you set up your API key above). Since DLWP and Pangu require different channels (and time steps), we will create two seperate data-sources for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state data/time\n",
    "time = datetime.datetime(2018, 1, 1)\n",
    "\n",
    "# DLWP datasource\n",
    "dlwp_data_source = cds.DataSource(dlwp_inference_model.in_channel_names)\n",
    "\n",
    "# Pangu datasource, this is much simplier since pangu only uses one timestep as an input\n",
    "pangu_data_source = cds.DataSource(pangu_inference_model.in_channel_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the initial state downloaded for each and set up in an Xarray dataset, we can now run deterministic inference for both which can be achieved using the `inference_ensemble.run_basic_inference` method which will produce a Xarray [data array](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.html) to then work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 12:48:24,887 INFO Welcome to the CDS\n",
      "2023-11-16 12:48:24,889 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-pressure-levels\n",
      "2023-11-16 12:48:24,899 INFO Welcome to the CDS\n",
      "2023-11-16 12:48:24,900 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n",
      "2023-11-16 12:48:25,185 INFO Request is queued\n",
      "2023-11-16 12:50:20,312 INFO Request is completed\n",
      "2023-11-16 12:50:20,314 INFO Downloading https://download-0010-clone.copernicus-climate.eu/cache-compute-0010/cache/data0/adaptor.mars.internal-1700139002.5754864-31898-7-cce79025-5e68-4c8f-a64a-91866ef2c9a2.grib to /root/.cache/earth2mip/cds/4d23315a3dd725f2ac8c7ecdd48b7525906b62eea6d2f8897734590532b42be2/reanalysis-era5-pressure-levels.grib.tmp (19.8M)\n",
      "2023-11-16 12:50:20,343 INFO Downloading https://download-0006-clone.copernicus-climate.eu/cache-compute-0006/cache/data5/adaptor.mars.internal-1700139004.5836535-6171-14-9b85dd73-a65a-4906-bf5b-faf1b0f578f5.grib to /root/.cache/earth2mip/cds/59bb55c852ba187dc020a159ba1e1b50f1bb1c8dc22309b8ef6994683f5439ac/reanalysis-era5-single-levels.grib.tmp (4M)\n",
      "2023-11-16 12:50:22,582 INFO Download rate 1.8M/s\n",
      "2023-11-16 12:50:24,266 INFO Download rate 5M/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray (time: 13, history: 1, channel: 7, lat: 721, lon: 1440)>\n",
      "array([[[[[ 2.50292755e+02,  2.50292755e+02,  2.50292755e+02, ...,\n",
      "            2.50292755e+02,  2.50292755e+02,  2.50292755e+02],\n",
      "          [ 2.50401154e+02,  2.50401154e+02,  2.50400177e+02, ...,\n",
      "            2.50401154e+02,  2.50401154e+02,  2.50401154e+02],\n",
      "          [ 2.50458771e+02,  2.50458771e+02,  2.50457794e+02, ...,\n",
      "            2.50458771e+02,  2.50458771e+02,  2.50458771e+02],\n",
      "          ...,\n",
      "          [ 2.59438263e+02,  2.59438263e+02,  2.59437286e+02, ...,\n",
      "            2.59440216e+02,  2.59439240e+02,  2.59439240e+02],\n",
      "          [ 2.59341583e+02,  2.59341583e+02,  2.59341583e+02, ...,\n",
      "            2.59340607e+02,  2.59340607e+02,  2.59341583e+02],\n",
      "          [ 2.59302521e+02,  2.59302521e+02,  2.59302521e+02, ...,\n",
      "            2.59302521e+02,  2.59302521e+02,  2.59302521e+02]],\n",
      "\n",
      "         [[ 5.56605469e+02,  5.56605469e+02,  5.56605469e+02, ...,\n",
      "            5.56605469e+02,  5.56605469e+02,  5.56605469e+02],\n",
      "          [ 5.47730469e+02,  5.47730469e+02,  5.47605469e+02, ...,\n",
      "            5.47855469e+02,  5.47855469e+02,  5.47730469e+02],\n",
      "          [ 5.40480469e+02,  5.40480469e+02,  5.40355469e+02, ...,\n",
      "            5.40730469e+02,  5.40730469e+02,  5.40605469e+02],\n",
      "...\n",
      "            1.69231987e+00,  1.69231987e+00,  1.69231987e+00],\n",
      "          [ 1.60725975e+00,  1.60725975e+00,  1.60725975e+00, ...,\n",
      "            1.69231987e+00,  1.69231987e+00,  1.69231987e+00],\n",
      "          [ 1.60725975e+00,  1.60725975e+00,  1.60725975e+00, ...,\n",
      "            1.69231987e+00,  1.69231987e+00,  1.69231987e+00]],\n",
      "\n",
      "         [[ 2.51248108e+02,  2.51248108e+02,  2.51248108e+02, ...,\n",
      "            2.47362411e+02,  2.47362411e+02,  2.47362411e+02],\n",
      "          [ 2.51248108e+02,  2.51248108e+02,  2.51248108e+02, ...,\n",
      "            2.47362411e+02,  2.47362411e+02,  2.47362411e+02],\n",
      "          [ 2.51248108e+02,  2.51248108e+02,  2.51248108e+02, ...,\n",
      "            2.47362411e+02,  2.47362411e+02,  2.47362411e+02],\n",
      "          ...,\n",
      "          [ 2.50676117e+02,  2.50676117e+02,  2.50676117e+02, ...,\n",
      "            2.51419312e+02,  2.51419312e+02,  2.51419312e+02],\n",
      "          [ 2.50676117e+02,  2.50676117e+02,  2.50676117e+02, ...,\n",
      "            2.51419312e+02,  2.51419312e+02,  2.51419312e+02],\n",
      "          [ 2.50676117e+02,  2.50676117e+02,  2.50676117e+02, ...,\n",
      "            2.51419312e+02,  2.51419312e+02,  2.51419312e+02]]]]],\n",
      "      dtype=float32)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0\n",
      "  * lon      (lon) float64 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.2 359.5 359.8\n",
      "  * channel  (channel) <U5 't850' 'z1000' 'z700' 'z500' 'z300' 'tcwv' 't2m'\n",
      "  * time     (time) datetime64[ns] 2018-01-01 2018-01-01T12:00:00 ... 2018-01-07\n",
      "Dimensions without coordinates: history\n"
     ]
    }
   ],
   "source": [
    "# Run DLWP inference\n",
    "dlwp_ds = inference_ensemble.run_basic_inference(\n",
    "    dlwp_inference_model,\n",
    "    n=12, # Note we run 12 steps here because DLWP is at 12 hour dt\n",
    "    data_source=dlwp_data_source,\n",
    "    time=time,\n",
    ")\n",
    "print(dlwp_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 12:50:47,289 INFO Welcome to the CDS\n",
      "2023-11-16 12:50:47,290 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n",
      "2023-11-16 12:50:47,292 INFO Welcome to the CDS\n",
      "2023-11-16 12:50:47,293 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-pressure-levels\n",
      "2023-11-16 12:50:47,551 INFO Request is queued\n",
      "2023-11-16 13:03:11,300 INFO Request is completed\n",
      "2023-11-16 13:03:11,302 INFO Downloading https://download-0021.copernicus-climate.eu/cache-compute-0021/cache/data2/adaptor.mars.internal-1700139757.7752109-20741-14-6c1da1f4-f748-484a-ba02-b2ffb23213e4.grib to /root/.cache/earth2mip/cds/edb75d3da65acf0c8da418e61717e8af311727bf261ba627fee9ec8cdd6a001f/reanalysis-era5-single-levels.grib.tmp (7.9M)\n",
      "2023-11-16 13:03:11,302 INFO Downloading https://download-0011-clone.copernicus-climate.eu/cache-compute-0011/cache/data5/adaptor.mars.internal-1700139759.1546063-9191-16-01fe3152-5463-41fe-a261-9322067ee138.grib to /root/.cache/earth2mip/cds/c0c6e2d51e65446f0f37bc4ccdba276e4ecd8532aea1b1981d89fdedcf57de29/reanalysis-era5-pressure-levels.grib.tmp (128.7M)\n",
      "2023-11-16 13:03:15,154 INFO Download rate 2.1M/s\n",
      "2023-11-16 13:03:22,109 INFO Download rate 11.9M/s\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:779: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray (time: 25, history: 1, channel: 69, lat: 721, lon: 1440)>\n",
      "array([[[[[ 5.56605469e+02,  5.56605469e+02,  5.56605469e+02, ...,\n",
      "            5.56605469e+02,  5.56605469e+02,  5.56605469e+02],\n",
      "          [ 5.47730469e+02,  5.47730469e+02,  5.47605469e+02, ...,\n",
      "            5.47855469e+02,  5.47855469e+02,  5.47730469e+02],\n",
      "          [ 5.40480469e+02,  5.40480469e+02,  5.40355469e+02, ...,\n",
      "            5.40730469e+02,  5.40730469e+02,  5.40605469e+02],\n",
      "          ...,\n",
      "          [-2.66445312e+01, -2.63945312e+01, -2.61445312e+01, ...,\n",
      "           -2.75195312e+01, -2.72695312e+01, -2.68945312e+01],\n",
      "          [-4.10195312e+01, -4.10195312e+01, -4.08945312e+01, ...,\n",
      "           -4.13945312e+01, -4.12695312e+01, -4.11445312e+01],\n",
      "          [-5.53945312e+01, -5.53945312e+01, -5.53945312e+01, ...,\n",
      "           -5.53945312e+01, -5.53945312e+01, -5.53945312e+01]],\n",
      "\n",
      "         [[ 6.15247510e+03,  6.15247510e+03,  6.15247510e+03, ...,\n",
      "            6.15247510e+03,  6.15247510e+03,  6.15247510e+03],\n",
      "          [ 6.15760010e+03,  6.15747510e+03,  6.15747510e+03, ...,\n",
      "            6.15772510e+03,  6.15772510e+03,  6.15760010e+03],\n",
      "          [ 6.16510010e+03,  6.16497510e+03,  6.16497510e+03, ...,\n",
      "            6.16535010e+03,  6.16535010e+03,  6.16522510e+03],\n",
      "...\n",
      "           -3.90969062e+00, -3.88687396e+00, -3.91723132e+00],\n",
      "          [-4.05871582e+00, -4.05376387e+00, -4.04933739e+00, ...,\n",
      "           -3.98225904e+00, -4.02659798e+00, -4.12245512e+00],\n",
      "          [-4.54896688e+00, -4.52407360e+00, -4.02886295e+00, ...,\n",
      "           -3.92184854e+00, -3.32457376e+00, -2.68053746e+00]],\n",
      "\n",
      "         [[ 2.44334518e+02,  2.44445969e+02,  2.44506134e+02, ...,\n",
      "            2.44988007e+02,  2.45070129e+02,  2.45231873e+02],\n",
      "          [ 2.44424988e+02,  2.44513458e+02,  2.44611282e+02, ...,\n",
      "            2.45077072e+02,  2.45260849e+02,  2.45316345e+02],\n",
      "          [ 2.44446457e+02,  2.44452148e+02,  2.44509430e+02, ...,\n",
      "            2.44895187e+02,  2.45075500e+02,  2.45186691e+02],\n",
      "          ...,\n",
      "          [ 2.49152130e+02,  2.49186417e+02,  2.49045105e+02, ...,\n",
      "            2.49449615e+02,  2.49300751e+02,  2.49235306e+02],\n",
      "          [ 2.49158997e+02,  2.48953369e+02,  2.48914993e+02, ...,\n",
      "            2.49157471e+02,  2.49187057e+02,  2.49226120e+02],\n",
      "          [ 2.71244568e+02,  2.69395630e+02,  2.70507660e+02, ...,\n",
      "            2.68905914e+02,  2.70006470e+02,  2.71984467e+02]]]]],\n",
      "      dtype=float32)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0\n",
      "  * lon      (lon) float64 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.2 359.5 359.8\n",
      "  * channel  (channel) <U5 'z1000' 'z925' 'z850' 'z700' ... 'u10m' 'v10m' 't2m'\n",
      "  * time     (time) datetime64[ns] 2018-01-01 2018-01-01T06:00:00 ... 2018-01-07\n",
      "Dimensions without coordinates: history\n"
     ]
    }
   ],
   "source": [
    "# Run Pangu inference\n",
    "pangu_ds = inference_ensemble.run_basic_inference(\n",
    "    pangu_inference_model,\n",
    "    n=24, # Note we run 24 steps here because Pangu is at 6 hour dt\n",
    "    data_source=pangu_data_source,\n",
    "    time=time,\n",
    ")\n",
    "print(pangu_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing\n",
    "\n",
    "With inference complete, now the fun part: post processing and analysis!\n",
    "Here we will just plot the z500 time-series of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get data-arrays at 12 hour steps\n",
    "dlwp_arr = dlwp_ds.sel(channel=\"z500\").values\n",
    "pangu_arr = pangu_ds.sel(channel=\"z500\").values[::2]\n",
    "# Plot\n",
    "fig, axs = plt.subplots(2, 13, figsize=(13*4, 5))\n",
    "for i in range(13):\n",
    "    axs[0,i].imshow(dlwp_arr[i,0])\n",
    "    axs[1,i].imshow(pangu_arr[i,0])\n",
    "    axs[0,i].set_title(time + datetime.timedelta(hours=12*i))\n",
    "\n",
    "axs[0,0].set_ylabel(\"DLWP\")\n",
    "axs[1,0].set_ylabel(\"Pangu\")\n",
    "plt.suptitle(\"z500 DLWP vs Pangu\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that completes the second notebook detailing how to run deterministic inference of two models using Earth-2 MIP. In the next notebook, we will dive deeper into how a PyTorch model is integrated into Earth-2 MIP. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
